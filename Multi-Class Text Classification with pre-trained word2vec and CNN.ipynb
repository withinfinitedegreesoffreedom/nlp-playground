{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Text Classification with word2vec and CNNs \n",
    "\n",
    "In this notebook, I train a classifier that can classify input text into 12 classes\n",
    "\n",
    "I use the pre-trained word2vec embeddings to generate the tokens from the text \n",
    "\n",
    "I then apply 1D convolutions of varying kernel sizes on the tokenized text to combine the pre-trained embeddings from word2vec which can help discover useful patterns in text. \n",
    "\n",
    "I then use a Softmax layer to classify the outcome of combining the outputs of several kernel convolutions to output the class probabilities for each instance. \n",
    "\n",
    "Reasons why this approach is chosen: \n",
    "- Plenty of data is available for training a NN\n",
    "- An attempt to train end-to-end and not worry about feature engineering\n",
    "- To use feature vectors of low dimensions such as 300 in this example vs. a few 100k features as in TfIdf vectorization\n",
    "- Curious how CNNs will learn the spatial depedency in the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import KeyedVectors\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading, tidying and sampling data \n",
    "\n",
    "In the below block, I perform the following functions. \n",
    "\n",
    "- Read the raw data from 'complaints_users.csv' and 'products.csv'\n",
    "- Merge the two tables on 'PRODUCT_ID' to get the raw text data and the class information into one frame\n",
    "- Drop columns that I won't be processing or using  \n",
    "- Merging classes\n",
    "- Sampling the majority classes to reduce the class imbalance\n",
    "- That creates a 'new_df' which has the necessary data for further processing\n",
    "\n",
    "## On merging classes: \n",
    "EDA has revealed that there are classes where the class name of one class is a sub-string of another class name. \n",
    "The below code snippet is an attempt to merge such classes.\n",
    "\n",
    "Below are the rules of the merger:\n",
    "- Class A's name should be fully contained in class B's name \n",
    "- Class A will be merged with Class B - this is to keep the longer class name intact as it has Class A's tags and much more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aws_install/.conda/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Input data into dataframes \n",
    "complaints_users = pd.read_csv('../data/complaints_users.csv')\n",
    "products = pd.read_csv('../data/products.csv')\n",
    "\n",
    "# Merge tables to create a unified dataset with predictors and response \n",
    "df = pd.merge(complaints_users, products, left_on=\"PRODUCT_ID\", right_on=\"PRODUCT_ID\", how=\"left\")\n",
    "\n",
    "# Drop columns that are not required\n",
    "df = df[[\"COMPLAINT_TEXT\", \"PRODUCT_ID\", \"MAIN_PRODUCT\", \"SUB_PRODUCT\"]]\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index()\n",
    "\n",
    "# Merging classes\n",
    "df.loc[df[\"MAIN_PRODUCT\"]==\"Credit card\", \"MAIN_PRODUCT\"] = \"Credit card or prepaid card\"\n",
    "df.loc[df[\"MAIN_PRODUCT\"]==\"Prepaid card\", \"MAIN_PRODUCT\"] = \"Credit card or prepaid card\"\n",
    "df.loc[df[\"MAIN_PRODUCT\"]==\"Payday loan\", \"MAIN_PRODUCT\"] = \"Payday loan, title loan, or personal loan\"\n",
    "df.loc[df[\"MAIN_PRODUCT\"]==\"Money transfers\", \"MAIN_PRODUCT\"] = \"Money transfer, virtual currency, or money service\"\n",
    "df.loc[df[\"MAIN_PRODUCT\"]==\"Virtual currency\", \"MAIN_PRODUCT\"] = \"Money transfer, virtual currency, or money service\"\n",
    "df.loc[df[\"MAIN_PRODUCT\"]==\"Credit reporting\", \"MAIN_PRODUCT\"] = \"Credit reporting, credit repair services, or other personal consumer reports\"\n",
    "\n",
    "# groupby \"main_products\" and perform majaority undersampling \n",
    "grouped_complaints = df.groupby(\"MAIN_PRODUCT\")\n",
    "new_df = pd.DataFrame()\n",
    "for name, group in grouped_complaints:\n",
    "    if group.shape[0] > 10000:\n",
    "        chosen_records = group.sample(n=10000, axis=0, random_state=9)\n",
    "    else:\n",
    "        chosen_records = group\n",
    "    new_df = pd.concat([new_df, chosen_records])\n",
    "\n",
    "# the new_df is ready\n",
    "new_df = df\n",
    "new_df = new_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text tidy tasks\n",
    "Cleaning text so that it is ready for further processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>COMPLAINT_TEXT</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>MAIN_PRODUCT</th>\n",
       "      <th>SUB_PRODUCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>xxxx transunion reporting incorrectly 120 day ...</td>\n",
       "      <td>26</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>xxxx transunion reporting incorrectly 120 day ...</td>\n",
       "      <td>26</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>xxxx xxxx experian need remove collection acco...</td>\n",
       "      <td>26</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3 company inconsistency violation double jeopa...</td>\n",
       "      <td>26</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>personal loan patriot finance incorrectly repo...</td>\n",
       "      <td>26</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346275</th>\n",
       "      <td>383059</td>\n",
       "      <td>used money gram send mother xxxx xxxx xxxx nev...</td>\n",
       "      <td>57</td>\n",
       "      <td>Money transfer, virtual currency, or money ser...</td>\n",
       "      <td>International money transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346276</th>\n",
       "      <td>383060</td>\n",
       "      <td>sent letter got response unfortunately victim ...</td>\n",
       "      <td>35</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>I do not know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346277</th>\n",
       "      <td>383062</td>\n",
       "      <td>requested credit score paid special reduced fe...</td>\n",
       "      <td>28</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346278</th>\n",
       "      <td>383064</td>\n",
       "      <td>originally attended xxxx xxxx located xxxx xxx...</td>\n",
       "      <td>91</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>Non-federal student loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346279</th>\n",
       "      <td>383065</td>\n",
       "      <td>attempted get inaccurate judgment removed equi...</td>\n",
       "      <td>28</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>346280 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                     COMPLAINT_TEXT  PRODUCT_ID  \\\n",
       "0            0  xxxx transunion reporting incorrectly 120 day ...          26   \n",
       "1            1  xxxx transunion reporting incorrectly 120 day ...          26   \n",
       "2            2  xxxx xxxx experian need remove collection acco...          26   \n",
       "3            3  3 company inconsistency violation double jeopa...          26   \n",
       "4            4  personal loan patriot finance incorrectly repo...          26   \n",
       "...        ...                                                ...         ...   \n",
       "346275  383059  used money gram send mother xxxx xxxx xxxx nev...          57   \n",
       "346276  383060  sent letter got response unfortunately victim ...          35   \n",
       "346277  383062  requested credit score paid special reduced fe...          28   \n",
       "346278  383064  originally attended xxxx xxxx located xxxx xxx...          91   \n",
       "346279  383065  attempted get inaccurate judgment removed equi...          28   \n",
       "\n",
       "                                             MAIN_PRODUCT  \\\n",
       "0       Credit reporting, credit repair services, or o...   \n",
       "1       Credit reporting, credit repair services, or o...   \n",
       "2       Credit reporting, credit repair services, or o...   \n",
       "3       Credit reporting, credit repair services, or o...   \n",
       "4       Credit reporting, credit repair services, or o...   \n",
       "...                                                   ...   \n",
       "346275  Money transfer, virtual currency, or money ser...   \n",
       "346276                                    Debt collection   \n",
       "346277  Credit reporting, credit repair services, or o...   \n",
       "346278                                       Student loan   \n",
       "346279  Credit reporting, credit repair services, or o...   \n",
       "\n",
       "                         SUB_PRODUCT  \n",
       "0                   Credit reporting  \n",
       "1                   Credit reporting  \n",
       "2                   Credit reporting  \n",
       "3                   Credit reporting  \n",
       "4                   Credit reporting  \n",
       "...                              ...  \n",
       "346275  International money transfer  \n",
       "346276                 I do not know  \n",
       "346277                           NaN  \n",
       "346278      Non-federal student loan  \n",
       "346279                           NaN  \n",
       "\n",
       "[346280 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some basic text tidy job is done here \n",
    "\n",
    "# regex to remove anything other than word and space - i.e, punctuations \n",
    "remove_punctuation = re.compile('[^\\w\\s]')\n",
    "\n",
    "# regex to remove xxxx usually credit card entries - do not use\n",
    "remove_xxxx = re.compile('\\sx+x')\n",
    "\n",
    "# regex to remove digits - do not use\n",
    "remove_digits = re.compile('\\d')\n",
    "\n",
    "# stopwords corpora \n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# this is a good lemmatizer that reduces nouns to their correct root form but leaves the verbs out\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "# this tokenizer splits not only on space but on punctuation too\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# function to clean the text\n",
    "def text_cleaning(text):\n",
    "    text = text.lower()\n",
    "    text = remove_punctuation.sub('', text)\n",
    "    #text = remove_xxxx.sub('', text)\n",
    "    #text = remove_digits.sub('', text)\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = ' '.join(stemmer.lemmatize(word) for word in text if word not in stopwords)\n",
    "    return text\n",
    "\n",
    "# Using apply to apply the above function on the COMPLAINT_TEXT series \n",
    "new_df[\"COMPLAINT_TEXT\"] = new_df[\"COMPLAINT_TEXT\"].apply(text_cleaning)\n",
    "\n",
    "# Below is an attempt to remove outlier text snippets that are too short \n",
    "lengths = new_df[\"COMPLAINT_TEXT\"].apply(lambda x: len(x))\n",
    "\n",
    "# short texts are those that have character count less than 100 - only for the purpose of this excercise \n",
    "short_texts = lengths[lengths < 100]\n",
    "\n",
    "# drop rows that have very short texts \n",
    "new_df.drop(short_texts.index, inplace=True)\n",
    "\n",
    "# this here is a spurious entry which has high character count but has absolutely no spaces \n",
    "# removing it\n",
    "new_df.drop([79984], inplace=True)\n",
    "new_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding class labels \n",
    "\n",
    "Class names are strings so far. Since I'm going to be using Cross Entropy Loss, I need the labels to be numeric. So I encode them to integers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the class labels to integers \n",
    "label_encoder = LabelEncoder()\n",
    "new_df[\"classes\"] = label_encoder.fit_transform(new_df[\"MAIN_PRODUCT\"])\n",
    "label_class_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading pre-trained embeddings \n",
    "\n",
    "Loading the slim Google news vectors below. I have downloaded them to /word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the word2vec embeddings for lookup \n",
    "lookup = KeyedVectors.load_word2vec_format('../word2vec/GoogleNews-vectors-negative300-SLIM.bin', \n",
    "                                                 binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocab: 299567\n",
      "\n",
      "Dimension of each word embedding: (300,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Making a note of all the available words in this embedding lookup\n",
    "available_words = []\n",
    "for word in lookup.vocab:\n",
    "    available_words.append(word)\n",
    "    \n",
    "# Information on embeddings and vocabulory \n",
    "print(\"Size of Vocab: {}\\n\".format(len(available_words)))\n",
    "print('Dimension of each word embedding: {}\\n'.format(lookup[available_words[0]].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "The lookup we created earlier holds a lot of words (those stored in \"available_words\") and their embeddings. Our corpus which is a list of complaints also will surely contain the same words, therefore embeddings are straighaway useful. \n",
    "\n",
    "To identify the words we have in our corpus with the words in pre-trained embeddings lookup, we assign to each word in our corpus, the identifier of our word in the pre-trained corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize our corpus\n",
    "def tokenize_complaints(lookup, complaint):\n",
    "    # here we can split on whitespace as previously we have already removed the punctuations \n",
    "    complaint_words = complaint.split(' ')\n",
    "    tokens = []\n",
    "    for word in complaint_words:\n",
    "        try:\n",
    "            idx = lookup.vocab[word].index\n",
    "        except:\n",
    "            idx = 0\n",
    "        tokens.append(idx)    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the tokenization function defined above to the series of our interest \n",
    "new_df[\"tokens\"] = new_df[\"COMPLAINT_TEXT\"].apply(lambda x: tokenize_complaints(lookup, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA has revealed that the mean length of text in corpus is ~110. The median is ~80 which is less than mean. We have \n",
    "# a distribution that is skewed right.  \n",
    "\n",
    "# To make the lengths of all of our text sequences identical, we pad the sequence with 0. 0 is also used while tokenization\n",
    "# to use as a token for \"unknown words(UNK)\" or words not present in \"available_words\" dictionary\n",
    "\n",
    "# max sequence length we would like our texts to have \n",
    "# texts with length > seq_length are truncated \n",
    "seq_length = 200\n",
    "\n",
    "# min length below which we pad the 0s on the left of the tokens\n",
    "min_length = 25\n",
    "\n",
    "# function to perform padding or truncation of tokens \n",
    "def pad_or_truncate_tokens(tokens):\n",
    "    n = len(tokens)\n",
    "    if n <= min_length:\n",
    "        pad_sequence = [0] * (seq_length-n)\n",
    "        pad_sequence.extend(tokens)\n",
    "        tokens = pad_sequence\n",
    "    elif n > seq_length:\n",
    "        tokens = tokens[0:seq_length]\n",
    "    elif n > min_length and n < seq_length:\n",
    "        pad_sequence = [0] * (seq_length-n)\n",
    "        tokens.extend(pad_sequence)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function defined above for token paddings / truncation \n",
    "new_df[\"padded_tokens\"] = new_df[\"tokens\"].apply(lambda x: pad_or_truncate_tokens(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we generate a numpy matrix of the tokens below \n",
    "features = np.zeros((new_df.shape[0], seq_length), dtype=int)\n",
    "\n",
    "for idx, row in enumerate(new_df[\"padded_tokens\"]):\n",
    "    for col, token in enumerate(row):\n",
    "        features[idx, col] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn's train_test_split with 'stratified sampling' so that the partitions have roughly the same \n",
    "# class balance as the original dataset\n",
    "X = features\n",
    "y = new_df[\"classes\"]\n",
    "\n",
    "# splitting into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=new_df[\"classes\"])\n",
    "\n",
    "# test is being split again into validation and test \n",
    "X_val, X_test, y_val, y_test =  train_test_split(X_test, y_test, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting numpy arrays to tensors for processing in Pytorch\n",
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train.to_numpy()))\n",
    "val_data = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val.to_numpy()))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test.to_numpy()))\n",
    "\n",
    "# batch size needs to be chosen before - keeping it low so that it is memory efficient \n",
    "batch_size = 256\n",
    "\n",
    "# batching data into three dataloaders to ensure data flow during train/val/test\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "valid_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ClassifierCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The embedding layer + CNN model that will be used to perform classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word2vec_lookup, vocab_size, output_size, embedding_dim,\n",
    "                 num_filters=100, kernel_sizes=[3, 4, 5, 6], freeze_embeddings=True, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(ClassifierCNN, self).__init__()\n",
    "\n",
    "        # set from input parameters \n",
    "        self.num_filters = num_filters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # set weights to pre-trained so that training is not needed, but they can be trained too\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(word2vec_lookup.vectors)) \n",
    "        if freeze_embeddings:\n",
    "            self.embedding.requires_grad = False\n",
    "        \n",
    "        # Convolutional layers - with kernel sizes [3,4,5] to cover 3,4,5 grams - totally 300 kernels \n",
    "        # padding is used to aid with edges (top & bottom)\n",
    "        self.convs_1d = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (k, embedding_dim), padding=(k-2,0)) \n",
    "            for k in kernel_sizes])\n",
    "        \n",
    "        # Fully-connected layer for classification\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_size) \n",
    "        \n",
    "        # Dropout and Softmax activation \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    \n",
    "    def convolution_and_pooling(self, x, conv):\n",
    "        \"\"\"\n",
    "        Convolutional + max pooling layer\n",
    "        \"\"\"\n",
    "        # squeeze last dim to get size: (batch_size, num_filters, conv_seq_length)\n",
    "        # conv_seq_length will be ~ 200\n",
    "        x = F.relu(conv(x)).squeeze(3)\n",
    "        \n",
    "        # 1D pool over conv_seq_length\n",
    "        # squeeze to get size: (batch_size, num_filters)\n",
    "        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines how a batch of inputs, x, passes through the model layers.\n",
    "        Returns a single, sigmoid-activated class score as output.\n",
    "        \"\"\"\n",
    "        # embedded vectors\n",
    "        embeddings = self.embedding(x) # (batch_size, seq_length, embedding_dim)\n",
    "        # embeds.unsqueeze(1) creates a channel dimension that conv layers expect\n",
    "        embeddings = embeddings.unsqueeze(1)\n",
    "        \n",
    "        # get output of each convolution-max-pooling layer\n",
    "        conv_output = [self.convolution_and_pooling(embeddings, conv) for conv in self.convs_1d]\n",
    "        \n",
    "        # concatenate results and add dropout\n",
    "        x = torch.cat(conv_output, 1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # apply linear layer \n",
    "        fc_out = self.fc(x) \n",
    "        \n",
    "        # softmax-activation for num_classes - np.argmax on axis=1 for class output\n",
    "        return self.softmax(fc_out)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierCNN(\n",
      "  (embedding): Embedding(299567, 300)\n",
      "  (convs_1d): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n",
      "    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1), padding=(2, 0))\n",
      "    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1), padding=(3, 0))\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=12, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Setting model hyper-parameters \n",
    "vocab_size = len(available_words)\n",
    "output_size = len(y.unique()) # num_classes after merging \n",
    "embedding_dim = len(lookup[available_words[0]]) # 300-dim vectors\n",
    "num_filters = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "\n",
    "model = ClassifierCNN(lookup, vocab_size, output_size, embedding_dim,\n",
    "                   num_filters, kernel_sizes)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001 # chosen empirically - could experiment if time permits\n",
    "\n",
    "# choosing loss criterion and optimizer \n",
    "# cross entropy loss as we have multiple classes\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the neural network\n",
    "def train(model, train_loader, epochs, print_every=100):\n",
    "\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        model.cuda()\n",
    "\n",
    "    counter = 0 # for printing\n",
    "    \n",
    "    # mark model for training \n",
    "    model.train()\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # looping through the batches from train_loader \n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # reset grads - this helps reset on each epoch\n",
    "            model.zero_grad()\n",
    "\n",
    "            # prediction from the model\n",
    "            output = model(inputs)\n",
    "\n",
    "            # calculate loss and back propogate \n",
    "            #print(output.shape)\n",
    "            #print(labels.shape)\n",
    "            loss = criterion(output.squeeze(), labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # validation \n",
    "            if counter % print_every == 0:\n",
    "                val_losses = []\n",
    "                \n",
    "                # mark the model for eval - so no gradients are accumulated or back propagated \n",
    "                model.eval()\n",
    "                for inputs, labels in valid_loader:\n",
    "\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                    \n",
    "                    # prediction on validation sample \n",
    "                    output = model(inputs)\n",
    "                    val_loss = criterion(output.squeeze(), labels.long())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                # Reset model for train for next epoch\n",
    "                model.train()\n",
    "                \n",
    "                # print stats \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs), \"Step: {}...\".format(counter), \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 100... Loss: 2.042921... Val Loss: 2.055054\n",
      "Epoch: 1/1... Step: 200... Loss: 1.976489... Val Loss: 1.970317\n",
      "Epoch: 1/1... Step: 300... Loss: 1.975717... Val Loss: 1.953417\n",
      "Epoch: 1/1... Step: 400... Loss: 1.949370... Val Loss: 1.926835\n",
      "Epoch: 1/1... Step: 500... Loss: 1.904049... Val Loss: 1.905294\n",
      "Epoch: 1/1... Step: 600... Loss: 1.915051... Val Loss: 1.904817\n",
      "Epoch: 1/1... Step: 700... Loss: 1.904727... Val Loss: 1.899374\n",
      "Epoch: 1/1... Step: 800... Loss: 1.920650... Val Loss: 1.894605\n",
      "Epoch: 1/1... Step: 900... Loss: 1.900077... Val Loss: 1.893631\n",
      "Epoch: 1/1... Step: 1000... Loss: 1.874893... Val Loss: 1.892006\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "num_epochs = 1\n",
    "print_every = 100\n",
    "\n",
    "train(model, train_loader, num_epochs, print_every=print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.895\n",
      "Test accuracy: 0.724\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output = model(inputs)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.long())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = np.argmax(output.detach().cpu(), axis=1)#torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.detach().cpu().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(lookup, model, complaint_text, pad_length=200):\n",
    "    \n",
    "    # mark the model for evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    # get the integer tokens from lookup\n",
    "    integer_tokens = tokenize_complaints(lookup, complaint_text)\n",
    "    \n",
    "    # pad or truncate the complaint if required \n",
    "    tokens = pad_or_truncate_tokens(integer_tokens)\n",
    "    \n",
    "    # convert the integer tokens to numpy features and then tensordata\n",
    "    features = np.zeros((1, len(tokens)), dtype=\"int\")\n",
    "    for i in range(len(features)):\n",
    "        features[i] = tokens[i]\n",
    "    features = torch.from_numpy(features)\n",
    "    \n",
    "    # pass the text through the model\n",
    "    batch_size = features.shape[0]\n",
    "    if train_on_gpu:\n",
    "        features = features.cuda()\n",
    "    output = model(features)\n",
    "    \n",
    "    # find the class predicted \n",
    "    pred = np.argmax(output.detach().cpu(), axis=1)\n",
    "    pred = np.array(pred)[0]\n",
    "    \n",
    "    # let's find the name of the class\n",
    "    predicted_class = label_class_mapping[pred]\n",
    "    \n",
    "    return predicted_class\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complaint text: received unsolicited credit card business ready activation upon arrival scary part actual card 15000000 limit addressed former board director active organization 15 year xxxx company never heard often receive solicitation credit application never one like would afraid someone could card like company credit headache would ensue thank help matter \n",
      "Predicted class: Credit card or prepaid card \n",
      "True class: Credit card or prepaid card\n"
     ]
    }
   ],
   "source": [
    "# some driver code for using predict \n",
    "random_sample = np.random.randint(0, new_df.shape[0])\n",
    "complaint_text = new_df.iloc[random_sample][\"COMPLAINT_TEXT\"]\n",
    "predicted_class = predict(lookup, model, complaint_text, pad_length=200)\n",
    "print(\"Complaint text: {}\".format(complaint_text), \"\\nPredicted class: {}\".format(predicted_class), \n",
    "      \"\\nTrue class: {}\".format(new_df.iloc[random_sample][\"MAIN_PRODUCT\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
