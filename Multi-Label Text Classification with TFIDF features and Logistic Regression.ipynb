{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Label Text Classification with TFIDF features and Logistic Regression\n",
    "\n",
    "In this notebook, I have implemented a Logistic Regression model for multi-label classification using TfIdf features extracted from text. \n",
    "\n",
    "Since the TFIDF vectorization is a very sparse matrix, linear classifiers are known to work well with them. Also the available data and the number of features extracted from them can although be large, linear methods can use them to train a model. \n",
    "\n",
    "I have therefore made the choice to use a Logistic regression model. \n",
    "\n",
    "I use a combination of main_product names and sub_product names as the tags after doing a number of text cleaning steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import tqdm\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, average_precision_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing data\n",
    "\n",
    "Below I prepare the data required for fitting Logistic Regression model. The main fields I use are the \"main_product\" as the class label and the \"complaint_text\" as the input text. \n",
    "\n",
    "After loading the data from tables, I merge the products and complaints tables to get the data into one frame. \n",
    "\n",
    "Later, I undersample the majority classes in order to reduce the class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data into dataframes \n",
    "complaints_users = pd.read_csv('../data/complaints_users.csv')\n",
    "products = pd.read_csv('../data/products.csv')\n",
    "\n",
    "# Merge tables to create a unified dataset with predictors and response \n",
    "df = pd.merge(complaints_users, products, left_on=\"PRODUCT_ID\", right_on=\"PRODUCT_ID\", how=\"left\")\n",
    "\n",
    "# Drop columns that are not required\n",
    "df = df[[\"COMPLAINT_TEXT\", \"PRODUCT_ID\", \"MAIN_PRODUCT\", \"SUB_PRODUCT\"]]\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index()\n",
    "\n",
    "# groupby \"main_products\" and perform majaority undersampling \n",
    "grouped_complaints = df.groupby(\"MAIN_PRODUCT\")\n",
    "new_df = pd.DataFrame()\n",
    "for name, group in grouped_complaints:\n",
    "    if group.shape[0] > 10000:\n",
    "        chosen_records = group.sample(n=10000, axis=0, random_state=9)\n",
    "    elif group.shape[0] > 5000 and group.shape[0] < 10000:\n",
    "        chosen_records = group\n",
    "    else:\n",
    "        pass\n",
    "    new_df = pd.concat([new_df, chosen_records])\n",
    "    \n",
    "# the new_df is ready\n",
    "new_df = new_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning \n",
    "\n",
    "Text tidying such as removal of punctuation, lemmatization of words after tokenization are performed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic text tidy job is done here \n",
    "\n",
    "# regex to remove anything other than word and space - i.e, punctuations \n",
    "remove_punctuation = re.compile('[^\\w\\s]')\n",
    "\n",
    "# regex to remove xxxx usually credit card entries - do not use\n",
    "remove_xxxx = re.compile('\\sx+x')\n",
    "\n",
    "# regex to remove digits - do not use\n",
    "remove_digits = re.compile('\\d')\n",
    "\n",
    "# stopwords corpora \n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# this is a good lemmatizer that reduces nouns to their correct root form but leaves the verbs out\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "# this tokenizer splits not only on space but on punctuation too\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# function to clean the text\n",
    "def text_cleaning(text):\n",
    "    text = text.lower()\n",
    "    text = remove_punctuation.sub('', text)\n",
    "    #text = remove_xxxx.sub('', text)\n",
    "    #text = remove_digits.sub('', text)\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = ' '.join(stemmer.lemmatize(word) for word in text if word not in stopwords)\n",
    "    return text\n",
    "\n",
    "# Using apply to apply the above function on the COMPLAINT_TEXT series \n",
    "new_df[\"COMPLAINT_TEXT\"] = new_df[\"COMPLAINT_TEXT\"].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some classes are merged due to the names being same or substrings of one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new_df.loc[new_df[\"MAIN_PRODUCT\"]==\"Credit card\", \"MAIN_PRODUCT\"] = \"Credit card or prepaid card\"\\nnew_df.loc[new_df[\"MAIN_PRODUCT\"]==\"Prepaid card\", \"MAIN_PRODUCT\"] = \"Credit card or prepaid card\"\\nnew_df.loc[new_df[\"MAIN_PRODUCT\"]==\"Payday loan\", \"MAIN_PRODUCT\"] = \"Payday loan, title loan, or personal loan\"\\nnew_df.loc[new_df[\"MAIN_PRODUCT\"]==\"Money transfers\", \"MAIN_PRODUCT\"] = \"Money transfer, virtual currency, or money service\"\\nnew_df.loc[new_df[\"MAIN_PRODUCT\"]==\"Virtual currency\", \"MAIN_PRODUCT\"] = \"Money transfer, virtual currency, or money service\"\\nnew_df.loc[new_df[\"MAIN_PRODUCT\"]==\"Credit reporting\", \"MAIN_PRODUCT\"] = \"Credit reporting, credit repair services, or other personal consumer reports'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging classes\n",
    "new_df.loc[new_df[\"MAIN_PRODUCT\"]==\"Credit card\", \"MAIN_PRODUCT\"] = \"Credit card or prepaid card\"\n",
    "new_df.loc[new_df[\"MAIN_PRODUCT\"]==\"Prepaid card\", \"MAIN_PRODUCT\"] = \"Credit card or prepaid card\"\n",
    "new_df.loc[new_df[\"MAIN_PRODUCT\"]==\"Payday loan\", \"MAIN_PRODUCT\"] = \"Payday loan, title loan, or personal loan\"\n",
    "new_df.loc[new_df[\"MAIN_PRODUCT\"]==\"Money transfers\", \"MAIN_PRODUCT\"] = \"Money transfer, virtual currency, or money service\"\n",
    "new_df.loc[new_df[\"MAIN_PRODUCT\"]==\"Virtual currency\", \"MAIN_PRODUCT\"] = \"Money transfer, virtual currency, or money service\"\n",
    "new_df.loc[new_df[\"MAIN_PRODUCT\"]==\"Credit reporting\", \"MAIN_PRODUCT\"] = \"Credit reporting, credit repair services, or other personal consumer reports\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition data and extract TFIDF features\n",
    "\n",
    "In this section, I will creates train/val/test partitions from data and then transform them to TFIDF features. \n",
    "\n",
    "X is the cleaned complained text from new_df. \n",
    "y (target) will be the tags obtained by concatenating the \"main_product\" and \"sub_product\" for each record. \n",
    "\n",
    "Sklearn train_test_split is used twice to progressively partition data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text on which feature extraction will be applied\n",
    "X = new_df[\"COMPLAINT_TEXT\"]\n",
    "\n",
    "# Target labels\n",
    "new_df[\"MAIN_PRODUCT\"] = new_df[\"MAIN_PRODUCT\"].fillna('')\n",
    "new_df[\"SUB_PRODUCT\"] = new_df[\"SUB_PRODUCT\"].fillna('')\n",
    "tags = pd.Series(zip(new_df[\"MAIN_PRODUCT\"], new_df[\"SUB_PRODUCT\"])).map(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two step application of data partitioning - the final data partition ratio will be 0.7:0.15:0.15\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, tags, test_size=0.3)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def extract_tfidf_features(X):\\n    tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.5, norm=\\'l2\\', ngram_range=(1, 2), stop_words=\"english\", max_features=10000)\\n    return tfidf_vectorizer.fit_transform(X)'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a function where I define a TFIDF vectorizer and fit it with train partition first and transform the two more partitions for use later in the pipeline.\n",
    "\n",
    "def extract_tfidf_features(X_train, X_test, X_val):\n",
    "    tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.5, norm='l2', ngram_range=(1, 2), stop_words=\"english\", max_features = 2**16)\n",
    "    tfidf_vectorizer.fit(X_train)\n",
    "    X_train = tfidf_vectorizer.transform(X_train)\n",
    "    X_val = tfidf_vectorizer.transform(X_val)\n",
    "    X_test = tfidf_vectorizer.transform(X_test)\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_train, X_val, y_train, y_val = train_test_split(X_tfidf, tags, test_size=0.3)\\n\\nX_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5)'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the above function to transform all three partitions to their feature forms \n",
    "X_train_tfidf, X_val_tfidf, X_test_tfidf = extract_tfidf_features(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "\n",
    "In this section I will define a Logistic Regression classifier which can predict tags for each text. \n",
    "\n",
    "Since each text snippet can have multiple labels assigned, during the prediction also there can be multiple tags predicted for each input text. \n",
    "\n",
    "To deal with this type of prediction of multiple tags per text, we need to prepare the \"target\" as a binary variable where 1 indicates the presence of the tag and 0 indicates it's absence. For each unique tag, there can be two possibilities - 1/0 for every text. \n",
    "\n",
    "Sklearn's MultiLabelBinarizer can encode the tags for each text in binary format as described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the MLB know of all the unique possible tags in the corpus \n",
    "\n",
    "# generate unique tags in the corpus\n",
    "# class tags counts\n",
    "class_tags_counts = {}\n",
    "for tag in tags:\n",
    "    for item in tag:\n",
    "        if item!=\"\":\n",
    "            if item in class_tags_counts:\n",
    "                class_tags_counts[item] += 1\n",
    "            else:\n",
    "                class_tags_counts[item] = 1\n",
    "    \n",
    "\n",
    "# create an instance of MLB with the unique tags we created above\n",
    "mlb = MultiLabelBinarizer(classes=sorted(class_tags_counts.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I transform the targets in train and val data into their binary counterparts - I have kept the \"test\" targets aside for now. \n",
    "y_train_tfidf = mlb.fit_transform(y_train)\n",
    "y_val_tfidf = mlb.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function below defines a classifier and fits it for training partition\n",
    "\n",
    "def train_classifier(X_train, y_train):\n",
    "    \"\"\"\n",
    "      X_train, y_train â€” training data\n",
    "      \n",
    "      return: trained classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create and fit LogisticRegression wraped into OneVsRestClassifier.\n",
    "\n",
    "    clf = OneVsRestClassifier(LogisticRegression(multi_class='ovr', penalty='l2', C=10)).fit(X_train, y_train)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I call the above function to fit the classifier with the training partition\n",
    "\n",
    "# Notice that the train X and train targets are tfidf transformed and binarized respectively\n",
    "classifier_tfidf = train_classifier(X_train_tfidf, y_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "\n",
    "In this section, I will evaluate the classifier on validation set and generate evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will predict the tags on the validation set and check if it performs okay. This is used to choose parameters of the Log reg model above.\n",
    "y_val_predicted_labels = classifier_tfidf.predict(X_val_tfidf)\n",
    "y_val_predicted_scores = classifier_tfidf.decision_function(X_val_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will reverse transform the predictions to check if they are good\n",
    "y_val_pred = mlb.inverse_transform(y_val_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complaint text: complaint regarding xxxx xxxx fka xxxx acctnoxxxx reporting experian xxxx xxxx credit bureausit special significance account disputed twice within last 30 day bureau bureau updated verified information time evidence attached 3rd party credit pull pulled today xxxx2019after multiple dispute verified bureau information match correct even though bureau say information correcti would like specious creditor bought account bank simply delete account solution ongoing problem proof clearly readable understandable bureau act unilaterally delete account creditor posse record original creditor merely guessing response dispute\n",
      "True labels: ['Credit reporting, credit repair services, or other personal consumer reports', 'Credit reporting']\n",
      "Predicted labels: ('Conventional adjustable mortgage (ARM)', 'Mortgage') \n",
      "\n",
      "complaint text: xxxxxxxx mailed xxxx xxxx check 55000 cover cost xxxx xxxx xxxx credit card account xxxx xxxx xxxx processed check yet send receipt send 3 credit reporting agency information disputed 3 credit reporting agency well providing substantial documentation yet party involved updated information credit file\n",
      "True labels: ['Credit reporting, credit repair services, or other personal consumer reports', 'Credit reporting']\n",
      "Predicted labels: ('Loan', 'Vehicle loan or lease') \n",
      "\n",
      "complaint text: hsbc repeatedly opened unrequested credit card mine andor husband name never requested authorized credit card despite numerous call complaint continue open account tie credit line time report matter treat fraud cover misdeed resolving fraud complaint ie card cancellation however problem recurring one fraud source misconduct rather salesmarketing group devised compensation plan drive reward criminal activity like xxxx xxxx hsbc card marketing group incentivizing employee performance plan based driving new cardholder volumemetrics please let know theyre getting away happy provide detail attached photo welcome letter new unrequested card identifying detail removed\n",
      "True labels: ['Credit card or prepaid card', 'General-purpose credit card or charge card']\n",
      "Predicted labels: ('Conventional fixed mortgage', 'Mortgage') \n",
      "\n",
      "complaint text: chapter xxxx bankruptcy bankruptcy complete xxxx showing 120 day overdue\n",
      "True labels: ['Credit reporting', '']\n",
      "Predicted labels: () \n",
      "\n",
      "complaint text: western union irresponsible calling back money transfer make deposited account date promised xxxx usd transferred still could trace money 5 day passed xxxx xxxxxxxx\n",
      "True labels: ['Money transfer, virtual currency, or money service', 'International money transfer']\n",
      "Predicted labels: ('Checking account',) \n",
      "\n",
      "complaint text: xxxxxxxx2015 account comerica bank charged xxxx xxxx 700 700 overdraft fee transaction pending 700 monthly service fee come account systematically xxxx every month 700 charge xxxx xxxx breakfast purchase xxxxxxxx2015 according ledger balance checking balance positive balance 1800 xxxxxxxx2015 figure double checked adding back pending transaction case something seen computer ledger according personal record balance accurate evening xxxxxxxx2015 xxxx xxxx charged 3100 clearly put account draft 3100 cleared bank 20000 deposit posted account xxxxxxxx2015 monday xxxx xxxx 2015 received letter stating account overdrawn pending charge left 200 enough prior xxxx pending charge 1500 700 700 charged 5000 despite fact 3100 charged evening xxxxxxxx15 clear xxxxxxxx2015 told xxxx representative enough cover 700 700 xxxxxxxx2015 3100 pending 700 700 cleared despite fact 700 pending since xxxxxxxx2015 700 pending since xxxxxxxx2015 transaction 3100 began pending xxxxxxxx2015 xxxx xxxx clearly 700 700 became transaction taken away available balance xxxxxxxx2015 customer service representative stated initial response take largest smallest transaction first comerica bank found guilty case xxxx v comerica bank na case xxxx u district court xxxx district florida part multidistrict litigation known checking account overdraft litigation case xxxx u district court xxxx district florida largest smallest applies comerica explain smallest posted xxxxxxxx largest xxxxxxxx next also told account subtract balance charge actually post account charge pending never charged account charged overdraft charge clearly would put account overdraft hold period 1 transaction 1 700 xxxxxxxx2015 cleared xxxxxxxx2015 2 transaction 2 700 xxxxxxxx2015 cleared xxxxxxxx2015 available balance xxxxxxxx2015 le transaction xxxx xxxx would 1800 3 transaction 3 3100 xxxxxxxx2015 cleared xxxxxxxx2015 4 transaction 4 xxxx xxxxxxxx2015 transaction xxxx xxxx overdraft account 3100 overdraft account agree according xxxx comerica representative transaction xxxx xxxx overdraft account according comerica philosophy account charged transaction cleared xxxx xxxx xxxx clear account certain pending charge ever become actual charge solidifying fact according comerica philosophy charge actual charge charge post account case enough money available xxxxxxxx2015 xxxxxxxx2015 cover charge respectively charge authorization date pending date xxxx overdraft charge received xxxx overdraft item visible account balance versus available balance following flow transaction xxxx xxxx would charge assessed account\n",
      "True labels: ['Bank account or service', 'Checking account']\n",
      "Predicted labels: ('Home equity loan or line of credit', 'Mortgage') \n",
      "\n",
      "complaint text: b mortgage modification loan place xxxx year reason b sending xxxx package home document state need sign document notarized document state home loan servicer pleased loan approved loan modificationalthough mortgage modified error format document used unable recorded county record office recorded properly homeowner may experience issue later attempting home loan serviced new loan modification copy previous loan modification later state reviewing document noticed xxxx item missing order recorded local county recorder deed requirement loan modification processbecause need assistance signing replacement agreement would like compensate time offering 50000 incentive checked county modification properly filed recorded documented county faxed document everything seems order called bank america many time regard keep giving reason assigned account specialist called back regularly tell answer b insists need resign taken b escalation department assigned another account specialist recontacted specialist last xxxx week least xxxx week tell answer need resign last call representative told would long able work need contact another person b supplied copy document deed registered county proof issue document indeed registered b continues xxxx document believe xxxx xxxx packet want straight answer b copy document county stating really issue document get straight answer anyone b going 6 month trust people want straight answer proof sleep night knowing people trying take home never missed payment never late please help\n",
      "True labels: ['Mortgage', 'FHA mortgage']\n",
      "Predicted labels: ('Conventional adjustable mortgage (ARM)', 'Mortgage') \n",
      "\n",
      "complaint text: never late mortgage payment long owned home roundpoint mortgage failed draft payment xxxxxxxx xxxxxxxx wwhich adversely affecting credit spoke numerous supervisor many time said would correct mistake remove two derogatory item three credit bureau account purchased another company roundpoint mortgage refusing help matter two year trying fix resolve matter\n",
      "True labels: ['Mortgage', 'FHA mortgage']\n",
      "Predicted labels: () \n",
      "\n",
      "complaint text: credit report displaying improper information disputed several time past 2 year consistently called disputed information xxxx credit burial information 8 year old law caused financial hardship currently still trying find away remove information account\n",
      "True labels: ['Credit reporting', '']\n",
      "Predicted labels: ('Checking account', 'Checking or savings account') \n",
      "\n",
      "complaint text: recently opened xxxx invest account want link capital one checking account purpose funding investment account talking capital one support confirmed currently blocking connection due security even though xxxx likely maintains equal greater security standard capital one\n",
      "True labels: ['Checking or savings account', 'Checking account']\n",
      "Predicted labels: ('Consumer Loan',) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I will check a few example predictions \n",
    "\n",
    "for i in range(10):\n",
    "    print(\"complaint text: {}\".format(X_val.iloc[i]))\n",
    "    print(\"True labels: {}\".format(y_val.iloc[i]))\n",
    "    print(\"Predicted labels: {} \\n\".format(y_val_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that prints performance measures\n",
    "def print_evaluation_scores(y_val, predicted):\n",
    "    \n",
    "    print(\"accuracy: {}\".format(accuracy_score(y_val, predicted)))\n",
    "    print(\"f1 score: {}\".format(f1_score(y_val, predicted,average=\"weighted\")))\n",
    "    print(\"average precision score: {}\".format(average_precision_score(y_val, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.033134143828559656\n",
      "f1 score: 0.09520909055244269\n",
      "average precision score: nan\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_scores(y_val_tfidf, y_val_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('py37': conda)",
   "language": "python",
   "name": "python37764bitpy37conda5df32f49df334c27ad67dd84df173b4f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
